Mon Oct 25 16:44:13 EDT 2021
35699738
Train unlearnable simclr with differentiable data augmentation. Larger step 3.2
python3 ssl_perturbation_save_model.py --config_path configs/cifar10 --exp_name path/to/your/experiment/folder --version resnet18 --train_data_type CIFAR10 --noise_shape 10 3 32 32 --epsilon 16 --num_steps 1 --step_size 3.2 --attack_type min-min --perturb_type classwise --universal_train_target 'classwise' --train_step 10 --epochs 151 --min_min_attack_fn non_eot
---------------------------------------------------------------
用的是8_1_0.8
Fri Nov  5 21:16:36 EDT 2021
103465371
Retrain SimCLR with perturbation from supervised
python simclr_transfer.py --batch_size 512 --epochs 1000 --arch resnet18 --perturbation_budget 16 --pre_load_name randomassign_supervised_10class_classwise_8_1_0.8 --job_id 103465371
---------------------------------------------------------------
Fri Nov  5 21:22:37 EDT 2021
103465373
Retrain SimCLR with perturbation from supervised
python simclr_transfer.py --batch_size 512 --epochs 1000 --arch resnet18 --perturbation_budget 32 --pre_load_name randomassign_supervised_10class_classwise_8_1_0.8 --job_id 103465373
---------------------------------------------------------------
Fri Nov  5 21:51:21 EDT 2021
103465376
Retrain SimCLR with perturbation from supervised
python simclr_transfer.py --batch_size 512 --epochs 1000 --arch resnet18 --perturbation_budget 8 --pre_load_name randomassign_supervised_10class_classwise_8_1_0.8 --job_id 103465376
---------------------------------------------------------------
Sat Nov  6 12:12:22 EDT 2021
103477543
86.37%
Retrain SimCLR with perturbation from supervised
python simclr_transfer.py --batch_size 512 --epochs 1000 --arch resnet18 --perturbation_budget 1 --pre_load_name randomassign_supervised_10class_classwise_8_1_0.8 --job_id 103477543
---------------------------------------------------------------

107351014
no shuffle
python3 -u ssl_perturbation_save_model.py --config_path configs/cifar10 --exp_name path/to/your/experiment/folder --version resnet18 --train_data_type CIFAR10 --noise_shape 1024 3 32 32 --epsilon 8 --num_steps 20 --step_size 0.8 --attack_type min-min --perturb_type samplewise --train_step 10 --epochs 1000 --min_min_attack_fn eot_v1 --strong_aug --class_4 --not_shuffle_train_data
107535314
samplewise_myshuffle 500 1000 shuffle_step
python3 -u ssl_perturbation_save_model.py --config_path configs/cifar10 --exp_name path/to/your/experiment/folder --version resnet18 --train_data_type CIFAR10 --noise_shape 1024 3 32 32 --epsilon 32 --num_steps 20 --step_size 0.8 --attack_type min-min --perturb_type samplewise --train_step 10 --epochs 1000 --min_min_attack_fn non_eot --strong_aug --class_4 --not_shuffle_train_data

51257955
用orgsample 只算dbindex进而生成的noise，看上去有点效果，但是它并没有考虑到transform，这样也可以吗？

51260064 说明只训练dbindex_loss没啥用

51310928 check shuffle 到底有没有影响（local run 同样的指令，前几个epoch看上去确实好像没影响，loss下降的也很快）



51166952 可以作为baseline
51203782 BPDA

51185289 4_class baseline
51030219 4_class baseline max_stage
50951206 4_class baseline
python3 -u ssl_perturbation_v2.py --config_path configs/cifar10 --exp_name path/to/your/experiment/folder --version resnet18 --train_data_type CIFAR10 --noise_shape 1024 3 32 32 --epsilon 8 --num_steps 20 --step_size 0.8 --attack_type min-min --perturb_type samplewise --train_step 10 --epochs 1000 --min_min_attack_fn eot_v1 --strong_aug --class_4 --not_shuffle_train_data --eot_size 10
51065227 4_class baseline mix_stage
python3 -u ssl_perturbation_v2.py --config_path configs/cifar10 --exp_name path/to/your/experiment/folder --version resnet18 --train_data_type CIFAR10 --noise_shape 1024 3 32 32 --epsilon 8 --num_steps 20 --step_size 0.8 --attack_type min-min --perturb_type samplewise_mix_stage --train_step 10 --epochs 1000 --min_min_attack_fn eot_v1 --strong_aug --class_4 --not_shuffle_train_data --eot_size 10

51211592 pytorch_aug baseline 可以通过分布式方式训练，速度比较快
51211597

51435720 之后的pytorch_aug 是直接从 data_loader当中增加的，应该是绝对正确的

51435697
51508204 baseline


51964797 clean_data supervised cifar10
51964801 simclrnoise supervised cifar10
51488092 clean cifar10

51611484 用samplewise的训练好的，对cluster 进行kmeans，然后按照kmeans的label进行classwise的结果

51508204 baseline，可以比较130 epoch的效果
52229020 baseline pytorch_aug

51029070 eot_10 simclr contrastive 效果特别好
51508207 同上


52332131 cifar100
52366249 eot cifar10 pytorch_aug

52327559 10、20 kmeans simclr_weight0
52366152 52351432

Submitted batch job 52428866 52428805 比较一下区别

52734683 good
cifar10 baseline
python3 -u ssl_perturbation_v2.py --piermaro_whole_epoch 51 --epochs 2 --config_path configs/cifar10 --exp_name path/to/your/experiment/folder --version resnet18 --train_data_type CIFAR10 --noise_shape 50000 3 32 32 --epsilon 8 --num_steps 20 --step_size 0.8 --attack_type min-min --perturb_type samplewise --train_step 20 --min_min_attack_fn eot_v1 --strong_aug --eot_size 1 --shuffle_train_perturb_data --pytorch_aug --simclr_weight 1 --not_use_normalized --linear_noise_dbindex_weight 0.1 --two_stage_PGD


supervised 52428805 52428866 52455797


52866270 52866443


53113236 two_stage_PGD 看上去还行 100kNN 37.86% suerpvised 33.20% 感觉这个水平就可以，但是不能稳定复现，很烦。
52866856 cifar100 这个看上去还可以
52894570

53319985 cifar10 moco

52321002 kmeans 有可能可以

CIFAR100
53317488 以及下边一组
3377
53332663 权重50

53729466 moco
53738139 53738140



CFIAR10
two_stage_PGD 52866443 0.06
52866132 0.08
53113232 0.045
52866270 0.06
Retrain SimCLR with perturbation from feature space
python3 ssl_perturbation_feature_space.py --pre_load_name differentiable_20211102231654_0.5_200_512 --noise_shape 1024 3 32 32 --epsilon 8 --num_steps 30 --step_size 0.4 --batch_size=512 --model_group 10 --job_id 103610131
check check
__name__ simclr
Files already downloaded and verified
Files already downloaded and verified
feature loss:  0.541972731705755
feature loss:  0.541316079441458
feature loss:  0.5412547839805484
feature loss:  0.5412338613532484
feature loss:  0.5412227706983685
feature loss:  0.5412158123217523
feature loss:  0.5412108725868165
feature loss:  0.5412072627805173
feature loss:  0.5412043416872621
feature loss:  0.5412018955685198
feature loss:  0.5411998056806624
feature loss:  0.5411980245262384
feature loss:  0.5411965283565223
feature loss:  0.5411952459253371
feature loss:  0.5411940822377801
feature loss:  0.5411930372938514
feature loss:  0.5411920873448253
feature loss:  0.5411912086419761
feature loss:  0.5411904486827552
feature loss:  0.541189641226083
feature loss:  0.5411890000104904
feature loss:  0.5411883350461721
feature loss:  0.5411877413280308
feature loss:  0.5411871951073408
feature loss:  0.5411866488866508
feature loss:  0.5411861739121377
feature loss:  0.541185675188899
feature loss:  0.5411852239631116
feature loss:  0.5411848202347755
feature loss:  0.5411844402551651
feature loss:  0.5411840840242803
feature loss:  0.5411836802959442
feature loss:  0.5411833715625107
feature loss:  0.5411830153316259
feature loss:  0.5411827065981925
feature loss:  0.541182397864759
feature loss:  0.5411821841262281
feature loss:  0.5411818753927946
feature loss:  0.5411816141568124
feature loss:  0.5411812816746533
feature loss:  0.5411810441873968
feature loss:  0.5411808304488659
feature loss:  0.541180616710335
feature loss:  0.5411804504692554
feature loss:  0.5411801654845476
feature loss:  0.5411799517460167
feature loss:  0.5411797380074859
feature loss:  0.5411795005202293
feature loss:  0.5411793342791498
feature loss:  0.5411791917867959
feature loss:  0.5411790255457163
feature loss:  0.5411788593046367
feature loss:  0.5411786930635571
feature loss:  0.5411785030737519
feature loss:  0.541178360581398
feature loss:  0.5411782418377697
feature loss:  0.5411780755966902
feature loss:  0.5411779093556106
feature loss:  0.541177814360708
feature loss:  0.5411776481196284
feature loss:  0.5411776006221771
feature loss:  0.5411773868836462
feature loss:  0.541177268140018
feature loss:  0.5411772206425667
feature loss:  0.5411770781502128
feature loss:  0.5411769831553102
feature loss:  0.5411768881604075
feature loss:  0.541176721919328
feature loss:  0.5411766269244254
feature loss:  0.5411765081807971
feature loss:  0.5411764131858945
feature loss:  0.5411762944422662
feature loss:  0.5411761994473636
feature loss:  0.5411760807037354
feature loss:  0.5411760094575584
feature loss:  0.5411758907139301
feature loss:  0.5411758432164788
feature loss:  0.5411757719703019
feature loss:  0.5411756769753993
feature loss:  0.541175629477948
feature loss:  0.5411755344830453
feature loss:  0.5411754394881427
feature loss:  0.5411753919906914
feature loss:  0.5411752494983375
feature loss:  0.5411751545034349
feature loss:  0.5411750832572579
feature loss:  0.5411750595085323
feature loss:  0.5411748932674527
feature loss:  0.5411748695187271
feature loss:  0.5411747745238245
feature loss:  0.5411747032776475
feature loss:  0.5411746557801962
feature loss:  0.5411745607852936
feature loss:  0.5411745132878423
feature loss:  0.5411744420416653
feature loss:  0.5411743470467627
feature loss:  0.541174323298037
feature loss:  0.5411742520518601
feature loss:  0.5411742045544088
feature loss:  0.5411741095595062
feature loss:  0.5411740620620549
feature loss:  0.5411739908158779
feature loss:  0.5411739670671523
feature loss:  0.541173919569701
feature loss:  0.5411738720722497
feature loss:  0.5411738008260727
feature loss:  0.541173777077347
feature loss:  0.5411736820824444
feature loss:  0.5411736345849931
feature loss:  0.5411735870875418
feature loss:  0.5411735158413649
feature loss:  0.5411734445951879
feature loss:  0.5411734208464622
feature loss:  0.5411733733490109
feature loss:  0.5411733258515596
feature loss:  0.5411732783541083
feature loss:  0.541173230856657
feature loss:  0.5411731833592057
feature loss:  0.5411731358617544
feature loss:  0.5411730646155775
feature loss:  0.5411730171181262
feature loss:  0.5411729221232235
feature loss:  0.5411729458719492
feature loss:  0.5411728746257722
feature loss:  0.5411728746257722
feature loss:  0.5411728033795953
feature loss:  0.541172755882144
feature loss:  0.541172755882144
feature loss:  0.5411726608872414
feature loss:  0.5411726371385157
feature loss:  0.5411725896410644
feature loss:  0.5411725658923388
feature loss:  0.5411725183948874
feature loss:  0.5411724708974361
feature loss:  0.5411724471487105
feature loss:  0.5411724471487105
feature loss:  0.5411723759025335
feature loss:  0.5411723521538079
feature loss:  0.5411723284050822
feature loss:  0.5411722809076309
feature loss:  0.5411721621640027
feature loss:  0.5411721621640027
feature loss:  0.541172138415277
feature loss:  0.5411720434203744
feature loss:  0.5411720434203744
feature loss:  0.5411720434203744
feature loss:  0.5411719959229231
feature loss:  0.5411719009280205
feature loss:  0.5411719009280205
feature loss:  0.5411719009280205
feature loss:  0.5411718296818435
feature loss:  0.5411717584356666
feature loss:  0.5411717584356666
feature loss:  0.5411717109382153
feature loss:  0.541171663440764
feature loss:  0.5411716396920383
feature loss:  0.5411716159433126
feature loss:  0.541171592194587
feature loss:  0.541171592194587
feature loss:  0.54117152094841
feature loss:  0.541171592194587
feature loss:  0.54117152094841
feature loss:  0.5411714497022331
feature loss:  0.5411714497022331
feature loss:  0.5411714497022331
feature loss:  0.5411714259535074
feature loss:  0.5411713547073305
feature loss:  0.5411713309586048
feature loss:  0.5411713072098792
feature loss:  0.5411712359637022
feature loss:  0.5411712834611535
feature loss:  0.5411712122149765
feature loss:  0.5411711884662509
feature loss:  0.5411712122149765
feature loss:  0.5411711409687996
feature loss:  0.5411711647175252
feature loss:  0.5411710934713483
feature loss:  0.5411710934713483
feature loss:  0.5411710697226226
feature loss:  0.5411710697226226
feature loss:  0.5411710222251713
feature loss:  0.5411709984764457
feature loss:  0.5411709984764457
feature loss:  0.5411709984764457
feature loss:  0.5411709272302687
feature loss:  0.5411708322353661
feature loss:  0.5411708797328174
feature loss:  0.5411708084866405
feature loss:  0.5411708084866405
feature loss:  0.5411707609891891
feature loss:  0.5411707609891891
feature loss:  0.5411706659942865
feature loss:  0.5411706422455609
feature loss:  0.5411706897430122
feature loss:  0.5411706422455609
feature loss:  0.5411706184968352
feature loss:  0.5411705947481096
feature loss:  0.5411705235019326
feature loss:  0.5411705472506583
feature loss:  0.5411705472506583
feature loss:  0.5411705235019326
feature loss:  0.541170499753207
feature loss:  0.5411704760044813
feature loss:  0.5411704522557557
feature loss:  0.5411704047583044
feature loss:  0.5411704047583044
feature loss:  0.541170357260853
feature loss:  0.5411703335121274
feature loss:  0.5411703335121274
feature loss:  0.5411702622659504
feature loss:  0.5411702622659504
feature loss:  0.5411702622659504
feature loss:  0.5411702385172248
feature loss:  0.5411702385172248
feature loss:  0.5411702147684991
feature loss:  0.5411701672710478
feature loss:  0.5411701910197735
feature loss:  0.5411701435223222
feature loss:  0.5411701435223222
feature loss:  0.5411700960248709
feature loss:  0.5411701197735965
feature loss:  0.5411700485274196
feature loss:  0.5411700960248709
feature loss:  0.5411700247786939
feature loss:  0.5411699772812426
feature loss:  0.5411700485274196
feature loss:  0.5411699772812426
feature loss:  0.541169953532517
feature loss:  0.5411699297837913
feature loss:  0.5411699297837913
feature loss:  0.5411699060350657
feature loss:  0.5411698585376143
feature loss:  0.5411698585376143
feature loss:  0.5411698347888887
feature loss:  0.5411698585376143
feature loss:  0.541169811040163
feature loss:  0.5411697872914374
feature loss:  0.5411697872914374
feature loss:  0.5411697872914374
feature loss:  0.5411697397939861
feature loss:  0.5411697160452604
feature loss:  0.5411697635427117
feature loss:  0.5411696447990835
feature loss:  0.5411696210503578
feature loss:  0.5411696685478091
feature loss:  0.5411695973016322
feature loss:  0.5411696447990835
feature loss:  0.5411695973016322
feature loss:  0.5411695973016322
feature loss:  0.5411695023067296
feature loss:  0.5411695498041809
feature loss:  0.5411695023067296
feature loss:  0.5411695023067296
feature loss:  0.5411694548092782
feature loss:  0.5411695260554552
feature loss:  0.5411694073118269
feature loss:  0.5411694073118269
feature loss:  0.5411693835631013
feature loss:  0.5411693835631013
feature loss:  0.54116933606565
feature loss:  0.54116933606565
feature loss:  0.54116933606565
feature loss:  0.54116933606565
feature loss:  0.5411693123169243
feature loss:  0.541169264819473
feature loss:  0.541169264819473
feature loss:  0.541169264819473
feature loss:  0.5411692410707474
feature loss:  0.5411692173220217
feature loss:  0.5411691935732961
feature loss:  0.5411691698245704
feature loss:  0.5411691698245704
feature loss:  0.5411691460758448
feature loss:  0.5411691223271191
feature loss:  0.5411691223271191
feature loss:  0.5411690985783935
feature loss:  0.5411690748296678
feature loss:  0.5411690510809422
feature loss:  0.5411690510809422
feature loss:  0.5411690510809422
feature loss:  0.5411690035834908
feature loss:  0.5411689798347652
feature loss:  0.5411690273322165
feature loss:  0.5411690035834908
feature loss:  0.5411689798347652
feature loss:  0.5411689085885882
feature loss:  0.5411689798347652
feature loss:  0.5411689323373139
feature loss:  0.5411689323373139
feature loss:  0.5411689085885882
feature loss:  0.5411688848398626
feature loss:  0.5411688610911369
feature loss:  0.5411688373424113
feature loss:  0.5411688610911369
feature loss:  0.5411687660962343
feature loss:  0.5411688135936856
feature loss:  0.5411687423475087
feature loss:  0.54116878984496
feature loss:  0.5411687660962343
feature loss:  0.5411687660962343
JobId=103610131 JobName=SimCLR
   UserId=renjie3(6007166) GroupId=cse(2010) MCS_label=N/A
   Priority=12117 Nice=0 Account=cmse QOS=normal
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=05:12:09 TimeLimit=1-06:00:00 TimeMin=N/A
   SubmitTime=2021-11-09T16:51:39 EligibleTime=2021-11-09T16:51:39
   AccrueTime=2021-11-09T16:51:39
   StartTime=2021-11-10T10:39:54 EndTime=2021-11-11T16:39:54 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2021-11-10T10:39:54
   Partition=cmse-gpu AllocNode:Sid=dev-amd20-v100:204881
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=nvl-001
   BatchHost=nvl-001
   FedOrigin=msuhpcc FedViableSiblings=msuhpcc,preemptable FedActiveSiblings=msuhpcc
   NumNodes=1 NumCPUs=2 NumTasks=1 CPUs/Task=2 ReqB:S:C:T=0:0:*:*
   TRES=cpu=2,mem=16G,node=1,billing=2490,gres/gpu=1
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=2 MinMemoryCPU=8G MinTmpDiskNode=0
   Features=[intel14|intel16|intel18|amr|nvf] DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/mnt/ufs18/home-145/renjie3/Documents/unlearnable/Unlearnable-Examples/submit.sb
   WorkDir=/mnt/ufs18/home-145/renjie3/Documents/unlearnable/Unlearnable-Examples
   Comment=stdout=/mnt/home/renjie3/Documents/unlearnable/Unlearnable-Examples/logfile/103610131.log 
   StdErr=/mnt/home/renjie3/Documents/unlearnable/Unlearnable-Examples/logfile/103610131.err
   StdIn=/dev/null
   StdOut=/mnt/home/renjie3/Documents/unlearnable/Unlearnable-Examples/logfile/103610131.log
   Power=
   TresPerNode=gpu:v100:1
   NtasksPerTRES:0


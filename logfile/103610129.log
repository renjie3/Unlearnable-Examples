Retrain SimCLR with perturbation from feature space
python3 ssl_perturbation_feature_space.py --pre_load_name differentiable_20211102231654_0.5_200_512 --noise_shape 1024 3 32 32 --epsilon 8 --num_steps 30 --step_size 0.8 --batch_size=512 --model_group 10 --job_id 103610129
check check
__name__ simclr
Files already downloaded and verified
Files already downloaded and verified
feature loss:  0.5436823074705899
feature loss:  0.5435513807460666
feature loss:  0.5435203649103642
feature loss:  0.5435052132233977
feature loss:  0.5434959274716675
feature loss:  0.543489539064467
feature loss:  0.5434847418218851
feature loss:  0.5434808470308781
feature loss:  0.5434778309427202
feature loss:  0.543475242331624
feature loss:  0.5434731049463153
feature loss:  0.5434712525457144
feature loss:  0.5434696138836443
feature loss:  0.5434681889601052
feature loss:  0.5434668590314686
feature loss:  0.5434656240977347
feature loss:  0.5434645554050803
feature loss:  0.5434635104611516
feature loss:  0.5434626317583025
feature loss:  0.543461776804179
feature loss:  0.5434609693475068
feature loss:  0.5434603043831885
feature loss:  0.5434595681726933
feature loss:  0.5434589744545519
feature loss:  0.5434584519825876
feature loss:  0.5434578582644463
feature loss:  0.5434573120437562
feature loss:  0.5434568370692432
feature loss:  0.5434563858434558
feature loss:  0.5434559108689427
feature loss:  0.5434554596431553
feature loss:  0.5434550559148192
feature loss:  0.5434546284377575
feature loss:  0.543454319704324
feature loss:  0.5434539397247136
feature loss:  0.5434536309912801
feature loss:  0.5434532985091209
feature loss:  0.5434530135244131
feature loss:  0.5434527047909796
feature loss:  0.5434523960575461
feature loss:  0.543452134821564
feature loss:  0.5434518260881305
feature loss:  0.5434515648521483
feature loss:  0.5434513511136174
feature loss:  0.5434511848725379
feature loss:  0.5434508761391044
feature loss:  0.5434506624005735
feature loss:  0.5434504011645913
feature loss:  0.5434501636773348
feature loss:  0.5434498549439013
feature loss:  0.5434497836977243
feature loss:  0.5434495699591935
feature loss:  0.543449189979583
feature loss:  0.5434490949846804
feature loss:  0.5434489049948752
feature loss:  0.5434486675076187
feature loss:  0.5434485250152647
feature loss:  0.5434483112767339
feature loss:  0.54344816878438
feature loss:  0.5434480737894773
feature loss:  0.5434478125534952
feature loss:  0.5434476938098669
feature loss:  0.543447480071336
feature loss:  0.5434474088251591
feature loss:  0.5434472900815308
feature loss:  0.5434470763429999
feature loss:  0.5434468863531947
feature loss:  0.5434468626044691
feature loss:  0.5434467201121151
feature loss:  0.5434465776197612
feature loss:  0.5434464826248586
feature loss:  0.543446316383779
feature loss:  0.5434462451376021
feature loss:  0.5434461263939738
feature loss:  0.5434460788965225
feature loss:  0.5434459839016199
feature loss:  0.543445841409266
feature loss:  0.5434457464143634
feature loss:  0.5434455564245582
feature loss:  0.5434455089271069
feature loss:  0.5434453901834786
feature loss:  0.5434453189373016
feature loss:  0.5434452476911247
feature loss:  0.5434451526962221
feature loss:  0.5434450102038682
feature loss:  0.5434449627064168
feature loss:  0.5434448677115142
feature loss:  0.5434448439627886
feature loss:  0.5434447727166116
feature loss:  0.5434446302242577
feature loss:  0.5434445589780807
feature loss:  0.5434443689882755
feature loss:  0.5434443927370012
feature loss:  0.5434443214908242
feature loss:  0.5434441789984703
feature loss:  0.543444131501019
feature loss:  0.5434440127573907
feature loss:  0.5434439177624881
feature loss:  0.5434438702650368
feature loss:  0.5434437990188599
feature loss:  0.5434437040239573
feature loss:  0.543443514034152
feature loss:  0.543443514034152
feature loss:  0.5434434427879751
feature loss:  0.5434433240443468
feature loss:  0.5434432765468955
feature loss:  0.5434431340545416
feature loss:  0.5434430628083646
feature loss:  0.543443039059639
feature loss:  0.5434430153109133
feature loss:  0.5434428965672851
feature loss:  0.5434427778236568
feature loss:  0.5434426590800285
feature loss:  0.5434426115825772
feature loss:  0.5434426353313029
feature loss:  0.543442492838949
feature loss:  0.5434424690902233
feature loss:  0.5434424690902233
feature loss:  0.5434423265978694
feature loss:  0.5434422553516924
feature loss:  0.5434422316029668
feature loss:  0.5434421366080642
feature loss:  0.5434420891106129
feature loss:  0.5434420891106129
feature loss:  0.5434419703669846
feature loss:  0.5434419703669846
feature loss:  0.543441875372082
feature loss:  0.543441804125905
feature loss:  0.5434417803771794
feature loss:  0.5434417566284537
feature loss:  0.5434415903873742
feature loss:  0.5434415666386485
feature loss:  0.5434416378848255
feature loss:  0.5434415666386485
feature loss:  0.5434414716437459
feature loss:  0.5434414716437459
feature loss:  0.5434413529001176
feature loss:  0.5434413766488433
feature loss:  0.5434413054026663
feature loss:  0.543441257905215
feature loss:  0.5434411629103124
feature loss:  0.5434411866590381
feature loss:  0.5434410916641355
feature loss:  0.5434410204179585
feature loss:  0.5434410441666842
feature loss:  0.5434409729205072
feature loss:  0.5434409491717815
feature loss:  0.5434409016743302
feature loss:  0.5434408779256046
feature loss:  0.5434408304281533
feature loss:  0.5434407591819763
feature loss:  0.5434407354332507
feature loss:  0.5434406641870737
feature loss:  0.5434406641870737
feature loss:  0.543440640438348
feature loss:  0.5434405929408967
feature loss:  0.5434405454434454
feature loss:  0.5434404979459941
feature loss:  0.5434404504485428
feature loss:  0.5434404266998172
feature loss:  0.5434404029510915
feature loss:  0.5434403792023659
feature loss:  0.5434403317049146
feature loss:  0.5434402842074633
feature loss:  0.5434403317049146
feature loss:  0.543440236710012
feature loss:  0.5434401179663837
feature loss:  0.5434401179663837
feature loss:  0.543440165463835
feature loss:  0.5434401417151093
feature loss:  0.5434400704689324
feature loss:  0.5434399754740298
feature loss:  0.5434400467202067
feature loss:  0.5434399279765785
feature loss:  0.5434399517253041
feature loss:  0.5434398329816759
feature loss:  0.5434398804791272
feature loss:  0.5434398567304015
feature loss:  0.5434398804791272
feature loss:  0.5434397142380476
feature loss:  0.5434397379867733
feature loss:  0.5434396429918706
feature loss:  0.543439690489322
feature loss:  0.5434395717456937
feature loss:  0.5434395954944193
feature loss:  0.5434395004995167
feature loss:  0.5434394055046141
feature loss:  0.5434394292533398
feature loss:  0.5434394530020654
feature loss:  0.5434392867609859
feature loss:  0.5434394055046141
feature loss:  0.5434393580071628
feature loss:  0.5434393105097115
feature loss:  0.5434392867609859
feature loss:  0.5434392630122602
feature loss:  0.5434391917660832
feature loss:  0.5434392392635345
feature loss:  0.5434392630122602
feature loss:  0.5434392392635345
feature loss:  0.5434391917660832
feature loss:  0.5434391442686319
feature loss:  0.5434391917660832
feature loss:  0.5434390967711806
feature loss:  0.5434390492737293
feature loss:  0.5434391205199063
feature loss:  0.5434390255250037
feature loss:  0.5434389542788267
feature loss:  0.5434389780275524
feature loss:  0.5434389542788267
feature loss:  0.5434388830326498
feature loss:  0.5434388355351985
feature loss:  0.5434388355351985
feature loss:  0.5434387405402958
feature loss:  0.5434387642890215
feature loss:  0.5434388355351985
feature loss:  0.5434386455453932
feature loss:  0.5434387405402958
feature loss:  0.5434386455453932
feature loss:  0.5434386692941189
feature loss:  0.5434386930428445
feature loss:  0.5434386455453932
feature loss:  0.5434386217966676
feature loss:  0.5434386455453932
feature loss:  0.5434385505504906
feature loss:  0.543438526801765
feature loss:  0.543438526801765
feature loss:  0.543438455555588
feature loss:  0.5434385505504906
feature loss:  0.5434384080581367
feature loss:  0.5434384080581367
feature loss:  0.5434383368119597
feature loss:  0.543438384309411
feature loss:  0.5434383605606854
feature loss:  0.5434383368119597
feature loss:  0.5434382893145084
feature loss:  0.5434382893145084
feature loss:  0.5434382893145084
feature loss:  0.5434382655657828
feature loss:  0.5434383368119597
feature loss:  0.5434382418170571
feature loss:  0.5434381468221545
feature loss:  0.5434381705708802
feature loss:  0.5434381705708802
feature loss:  0.5434381230734289
feature loss:  0.5434381705708802
feature loss:  0.5434380280785263
feature loss:  0.5434381230734289
feature loss:  0.543437980581075
feature loss:  0.5434380755759776
feature loss:  0.5434380280785263
feature loss:  0.5434381468221545
feature loss:  0.543437909334898
feature loss:  0.5434379568323493
feature loss:  0.5434378855861723
feature loss:  0.543437980581075
feature loss:  0.5434379330836236
feature loss:  0.5434379568323493
feature loss:  0.543437909334898
feature loss:  0.5434378618374467
feature loss:  0.543437838088721
feature loss:  0.5434378143399954
feature loss:  0.5434377905912697
feature loss:  0.543437838088721
feature loss:  0.5434377668425441
feature loss:  0.5434377430938184
feature loss:  0.5434377905912697
feature loss:  0.5434377668425441
feature loss:  0.5434376480989158
feature loss:  0.5434377193450928
feature loss:  0.5434376955963671
feature loss:  0.5434376718476415
feature loss:  0.5434376243501902
feature loss:  0.5434376243501902
feature loss:  0.5434376006014645
feature loss:  0.5434375293552876
feature loss:  0.5434375531040132
feature loss:  0.5434375531040132
feature loss:  0.5434375293552876
feature loss:  0.5434375056065619
feature loss:  0.5434375056065619
feature loss:  0.5434374343603849
feature loss:  0.5434375293552876
feature loss:  0.543437363114208
feature loss:  0.5434374343603849
feature loss:  0.5434373868629336
feature loss:  0.5434374343603849
feature loss:  0.5434373156167567
feature loss:  0.5434373868629336
feature loss:  0.543437363114208
feature loss:  0.5434374106116593
feature loss:  0.543437291868031
feature loss:  0.5434373868629336
feature loss:  0.5434373156167567
feature loss:  0.5434373156167567
feature loss:  0.5434372443705797
feature loss:  0.5434372681193054
feature loss:  0.5434372443705797
feature loss:  0.5434372681193054
feature loss:  0.5434372206218541
feature loss:  0.5434372206218541
JobId=103610129 JobName=SimCLR
   UserId=renjie3(6007166) GroupId=cse(2010) MCS_label=N/A
   Priority=11656 Nice=0 Account=cmse QOS=normal
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=05:07:42 TimeLimit=1-06:00:00 TimeMin=N/A
   SubmitTime=2021-11-09T16:51:21 EligibleTime=2021-11-09T16:51:21
   AccrueTime=2021-11-09T16:51:21
   StartTime=2021-11-10T09:37:57 EndTime=2021-11-11T15:37:57 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2021-11-10T09:37:57
   Partition=cmse-gpu AllocNode:Sid=dev-amd20-v100:204881
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=nvl-001
   BatchHost=nvl-001
   FedOrigin=msuhpcc FedViableSiblings=msuhpcc,preemptable FedActiveSiblings=msuhpcc
   NumNodes=1 NumCPUs=2 NumTasks=1 CPUs/Task=2 ReqB:S:C:T=0:0:*:*
   TRES=cpu=2,mem=16G,node=1,billing=2490,gres/gpu=1
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=2 MinMemoryCPU=8G MinTmpDiskNode=0
   Features=[intel14|intel16|intel18|amr|nvf] DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/mnt/ufs18/home-145/renjie3/Documents/unlearnable/Unlearnable-Examples/submit.sb
   WorkDir=/mnt/ufs18/home-145/renjie3/Documents/unlearnable/Unlearnable-Examples
   Comment=stdout=/mnt/home/renjie3/Documents/unlearnable/Unlearnable-Examples/logfile/103610129.log 
   StdErr=/mnt/home/renjie3/Documents/unlearnable/Unlearnable-Examples/logfile/103610129.err
   StdIn=/dev/null
   StdOut=/mnt/home/renjie3/Documents/unlearnable/Unlearnable-Examples/logfile/103610129.log
   Power=
   TresPerNode=gpu:v100:1
   NtasksPerTRES:0


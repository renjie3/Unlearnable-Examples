New small dataset
python3 -u ssl_perturbation_save_model_random_noise.py --config_path configs/cifar10 --exp_name path/to/your/experiment/folder --version resnet18 --train_data_type CIFAR10 --noise_shape 4 3 32 32 --epsilon 64 --num_steps 20 --step_size 3.2 --attack_type min-min --perturb_type classwise --universal_train_target 'classwise' --train_step 10 --epochs 1000 --min_min_attack_fn non_eot --strong_aug --random_start
check check
__name__ simclr
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
tensor([[[[-0.0357,  0.0673, -0.0657,  ...,  0.1770,  0.1982,  0.1892],
          [-0.2429,  0.1522, -0.0808,  ..., -0.0427, -0.1806, -0.1221],
          [ 0.0519, -0.1897,  0.1863,  ..., -0.1577, -0.1104,  0.0198],
          ...,
          [-0.1086,  0.1971, -0.0021,  ..., -0.1624, -0.2395, -0.0063],
          [-0.2464,  0.2450,  0.1066,  ...,  0.0394,  0.1349,  0.1505],
          [ 0.0395, -0.1456,  0.0239,  ...,  0.0362,  0.1386,  0.0828]],

         [[-0.1518,  0.1297,  0.1891,  ...,  0.0605, -0.1073,  0.0677],
          [-0.0293, -0.2428, -0.2028,  ...,  0.0852,  0.1236,  0.1042],
          [ 0.1463, -0.1421, -0.0680,  ...,  0.0743,  0.1857,  0.0800],
          ...,
          [ 0.2222,  0.0773, -0.0770,  ...,  0.0379, -0.1359, -0.2284],
          [-0.2453,  0.2373,  0.1889,  ..., -0.1983,  0.0375, -0.1927],
          [ 0.0242, -0.1776, -0.1019,  ..., -0.1251,  0.1604, -0.0567]],

         [[-0.1136,  0.1642, -0.1774,  ...,  0.2182, -0.1528,  0.1844],
          [-0.2360,  0.0966,  0.1229,  ...,  0.1733, -0.0234, -0.0775],
          [ 0.2026,  0.1905,  0.2343,  ...,  0.0058,  0.2061,  0.1246],
          ...,
          [ 0.1681,  0.1476,  0.0802,  ...,  0.2086,  0.2137, -0.0888],
          [-0.2071,  0.1986,  0.1471,  ..., -0.1181,  0.1409,  0.1435],
          [ 0.1326, -0.0484,  0.2122,  ..., -0.1209,  0.2130, -0.2346]]],


        [[[-0.1956, -0.2262, -0.0640,  ..., -0.2107,  0.0595,  0.1501],
          [ 0.1421, -0.0597,  0.0519,  ..., -0.0197, -0.0470, -0.1319],
          [ 0.1119, -0.0995, -0.2356,  ..., -0.0498, -0.0761, -0.0023],
          ...,
          [ 0.1968,  0.0981,  0.1419,  ...,  0.0039,  0.2212, -0.2055],
          [-0.0907, -0.0170,  0.1045,  ...,  0.2154,  0.2408, -0.1597],
          [-0.1216,  0.0418,  0.2501,  ...,  0.1934,  0.0445, -0.1692]],

         [[-0.0998,  0.2500, -0.1986,  ..., -0.2003,  0.1874,  0.0278],
          [-0.2087,  0.0573, -0.1473,  ...,  0.1349,  0.0987, -0.0357],
          [ 0.0047,  0.0659, -0.0502,  ...,  0.2474,  0.1348, -0.2008],
          ...,
          [-0.2073,  0.0744, -0.0931,  ..., -0.2411, -0.0192, -0.0345],
          [ 0.1719,  0.0381, -0.0753,  ...,  0.0975,  0.2061,  0.2101],
          [-0.0369, -0.1510,  0.2315,  ..., -0.0391,  0.0084,  0.0691]],

         [[ 0.0891,  0.2085, -0.0850,  ..., -0.2474,  0.1579, -0.1669],
          [-0.0008, -0.2341, -0.0262,  ...,  0.0850,  0.0550, -0.1043],
          [-0.0565,  0.0538, -0.1764,  ...,  0.2297,  0.2048, -0.1189],
          ...,
          [ 0.0830, -0.2012, -0.0256,  ...,  0.2143, -0.1420,  0.0733],
          [ 0.1315, -0.0449,  0.2160,  ..., -0.1141,  0.2022, -0.0473],
          [-0.0693, -0.1730,  0.0589,  ...,  0.1896, -0.0062, -0.1864]]],


        [[[-0.1516, -0.0177, -0.0304,  ...,  0.1734, -0.0205, -0.1313],
          [ 0.1353, -0.1124,  0.1406,  ...,  0.2139,  0.2012,  0.2356],
          [-0.2059,  0.1023,  0.2052,  ...,  0.0416, -0.1084,  0.2016],
          ...,
          [-0.0095,  0.0237, -0.0059,  ...,  0.0211, -0.0079, -0.1232],
          [ 0.2099, -0.1676,  0.1959,  ..., -0.2100, -0.2394, -0.1910],
          [-0.0975,  0.0834, -0.0472,  ...,  0.0153,  0.1461, -0.2111]],

         [[ 0.0522, -0.1349, -0.0765,  ...,  0.0598, -0.0941, -0.0835],
          [-0.1238,  0.0692,  0.0462,  ...,  0.2250, -0.0426, -0.0807],
          [-0.2039, -0.2188,  0.0329,  ...,  0.0107, -0.0587, -0.1247],
          ...,
          [-0.1679,  0.0351, -0.0102,  ...,  0.2104, -0.2353,  0.0489],
          [ 0.1713, -0.0712,  0.1783,  ..., -0.1132,  0.0282, -0.0175],
          [ 0.0258, -0.0028,  0.0424,  ..., -0.0049, -0.2039,  0.0350]],

         [[ 0.0098, -0.0915,  0.2116,  ..., -0.0154,  0.2450,  0.2176],
          [ 0.1005, -0.1581,  0.1467,  ...,  0.1893, -0.1001,  0.0521],
          [-0.1807,  0.1478,  0.2091,  ..., -0.0670,  0.0936, -0.2156],
          ...,
          [-0.1696, -0.1260, -0.2101,  ..., -0.2088, -0.0276, -0.2215],
          [-0.2344, -0.1793, -0.2353,  ..., -0.0166, -0.0180,  0.0423],
          [ 0.2448, -0.1912,  0.1627,  ...,  0.1301,  0.1146,  0.1861]]],


        [[[-0.2489, -0.0619, -0.0942,  ..., -0.1662,  0.1583,  0.0638],
          [-0.0040,  0.2392,  0.1646,  ..., -0.2504, -0.2077, -0.0445],
          [-0.1180,  0.1826, -0.1241,  ...,  0.2147,  0.1399,  0.2435],
          ...,
          [ 0.0432, -0.1836, -0.0236,  ...,  0.2240,  0.1575, -0.2346],
          [-0.1568,  0.1695, -0.2213,  ..., -0.1652, -0.2129,  0.1900],
          [-0.2227,  0.1177, -0.0021,  ...,  0.0290, -0.0508, -0.2251]],

         [[-0.1693,  0.1660,  0.1782,  ...,  0.2374, -0.1823,  0.2349],
          [ 0.0743, -0.1243, -0.2217,  ..., -0.2368,  0.2185, -0.0851],
          [ 0.2400,  0.1029,  0.1448,  ...,  0.0584,  0.1300, -0.1927],
          ...,
          [ 0.1202,  0.0237, -0.1947,  ..., -0.2457, -0.0317, -0.0837],
          [-0.0597,  0.1913, -0.1953,  ..., -0.0750,  0.1619, -0.0526],
          [ 0.1707,  0.1258,  0.1954,  ...,  0.0344, -0.0288, -0.0734]],

         [[ 0.2071,  0.1931, -0.0318,  ...,  0.1715,  0.0039,  0.1216],
          [ 0.1735,  0.1188,  0.0177,  ...,  0.0103,  0.2273, -0.2292],
          [ 0.2245,  0.0447,  0.0071,  ...,  0.1832,  0.0219, -0.0910],
          ...,
          [ 0.1883, -0.1714, -0.0372,  ..., -0.0531, -0.2405, -0.0510],
          [ 0.0070, -0.2499, -0.0014,  ...,  0.1300, -0.0869, -0.0489],
          [-0.0371, -0.1509,  0.1451,  ..., -0.2276,  0.2104,  0.1363]]]],
       device='cuda:0')
The whole epochs are 1000
6.961974859237671
6.882002115249634
6.874514579772949
6.883124589920044
6.836381435394287
6.857667446136475
6.806478500366211
6.831032752990723
6.824361801147461
6.795636415481567
model saved at unlearnable_20211031215110_0.5_512_1000
6.8016273975372314
6.786715984344482
6.8125340938568115
6.8102052211761475
6.8006591796875
6.795679092407227
6.771294116973877
6.747465372085571
6.793891906738281
6.767369031906128
model saved at unlearnable_20211031215110_0.5_512_1000
6.788555383682251
6.843120336532593
6.790596008300781
6.766443967819214
6.767415761947632
6.797682762145996
6.756295204162598
6.747912645339966
6.731524467468262
6.7702577114105225
model saved at unlearnable_20211031215110_0.5_512_1000
6.770187854766846
6.7648820877075195
6.746546983718872
6.74027681350708
6.72545313835144
6.777550458908081
6.728943586349487
6.745063781738281
6.720489740371704
6.690499305725098
model saved at unlearnable_20211031215110_0.5_512_1000
6.720407247543335
6.668240070343018
6.714604139328003
6.709888935089111
6.704063415527344
6.700240850448608
6.689917325973511
6.720407485961914
6.7387800216674805
6.710411071777344
model saved at unlearnable_20211031215110_0.5_512_1000
6.678314208984375
6.7013959884643555
6.7266623973846436
6.685918807983398
6.691635370254517
6.697162866592407
6.699161529541016
6.666427373886108
6.712937831878662
6.672688961029053
model saved at unlearnable_20211031215110_0.5_512_1000
6.681675910949707
6.661083936691284
6.670597314834595
6.646130800247192
6.666064262390137
6.6833930015563965
6.683188438415527
6.614920139312744
6.6458330154418945
6.6363749504089355
model saved at unlearnable_20211031215110_0.5_512_1000
6.601713180541992
6.619863510131836
6.6451544761657715
6.60865592956543
6.637907028198242
6.6425862312316895
6.632652282714844
6.5812273025512695
6.638312578201294
6.631548166275024
model saved at unlearnable_20211031215110_0.5_512_1000
6.639688968658447
6.6539459228515625
6.603928804397583
6.611652135848999
6.587658643722534
6.573418855667114
6.604247570037842
6.575924873352051
6.592290163040161
6.538457870483398
model saved at unlearnable_20211031215110_0.5_512_1000
6.597918510437012
6.642512798309326
6.584152698516846
6.562966346740723
6.5955891609191895
6.586296319961548
6.579310178756714
6.563627243041992
6.53058123588562
6.524167537689209
model saved at unlearnable_20211031215110_0.5_512_1000
6.590624094009399
6.549641847610474
6.542743682861328
6.522516250610352
6.538217067718506
6.543134689331055
6.539114475250244
6.5449981689453125
6.614208459854126
6.576319694519043
model saved at unlearnable_20211031215110_0.5_512_1000
6.547255754470825
6.626973867416382
6.549072027206421
6.534703016281128
6.500165700912476
6.565097808837891
6.565192461013794
6.544945001602173
6.532908916473389
6.536967754364014
model saved at unlearnable_20211031215110_0.5_512_1000
6.550768613815308
6.539809703826904
6.529965877532959
6.528477191925049
6.514610052108765
6.5089757442474365
6.504900693893433
6.5042078495025635
6.52740216255188
6.544438123703003
model saved at unlearnable_20211031215110_0.5_512_1000
6.47306489944458
6.47152853012085
6.522152900695801
6.534992456436157
6.475137710571289
6.479849338531494
6.4887285232543945
6.487383127212524
6.5394744873046875
6.487037658691406
model saved at unlearnable_20211031215110_0.5_512_1000
6.498986482620239
6.504374027252197
6.536102056503296
6.490034818649292
6.515189170837402
6.524441957473755
6.481499433517456
6.50495982170105
6.494204998016357
6.496270656585693
model saved at unlearnable_20211031215110_0.5_512_1000
6.486173152923584
6.494540214538574
6.489008903503418
6.497894287109375

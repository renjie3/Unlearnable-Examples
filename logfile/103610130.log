Retrain SimCLR with perturbation from feature space
python3 ssl_perturbation_feature_space.py --pre_load_name differentiable_20211102231654_0.5_200_512 --noise_shape 1024 3 32 32 --epsilon 8 --num_steps 50 --step_size 0.8 --batch_size=512 --model_group 10 --job_id 103610130
check check
__name__ simclr
Files already downloaded and verified
Files already downloaded and verified
feature loss:  0.5368311088532209
feature loss:  0.5367821152321994
feature loss:  0.5367669397965074
feature loss:  0.5367586752399802
feature loss:  0.5367534030228853
feature loss:  0.5367496032267809
feature loss:  0.5367467058822513
feature loss:  0.5367445209994912
feature loss:  0.5367426685988903
feature loss:  0.5367411011829972
feature loss:  0.5367397000081837
feature loss:  0.536738583818078
feature loss:  0.536737562622875
feature loss:  0.5367366364225745
feature loss:  0.5367356864735484
feature loss:  0.5367349265143275
feature loss:  0.5367343327961862
feature loss:  0.5367336915805936
feature loss:  0.5367330741137266
feature loss:  0.5367324566468596
feature loss:  0.5367319816723466
feature loss:  0.5367314829491079
feature loss:  0.5367309604771435
feature loss:  0.5367305330000818
feature loss:  0.5367301530204713
feature loss:  0.5367297492921352
feature loss:  0.5367294643074274
feature loss:  0.536729084327817
feature loss:  0.5367288230918348
feature loss:  0.53672846686095
feature loss:  0.5367280631326139
feature loss:  0.5367278731428087
feature loss:  0.5367275169119239
feature loss:  0.5367272794246674
feature loss:  0.5367270181886852
feature loss:  0.5367268044501543
feature loss:  0.5367264957167208
feature loss:  0.53672628197819
feature loss:  0.5367261157371104
feature loss:  0.5367258307524025
feature loss:  0.5367257120087743
feature loss:  0.5367255457676947
feature loss:  0.5367253795266151
feature loss:  0.5367252132855356
feature loss:  0.5367250707931817
feature loss:  0.536724975798279
feature loss:  0.5367248095571995
feature loss:  0.5367246670648456
feature loss:  0.5367245483212173
feature loss:  0.5367243345826864
feature loss:  0.5367242158390582
feature loss:  0.5367240258492529
feature loss:  0.5367239071056247
feature loss:  0.5367237883619964
feature loss:  0.5367236696183681
feature loss:  0.5367235033772886
feature loss:  0.5367234321311116
feature loss:  0.5367233846336603
feature loss:  0.5367231946438551
feature loss:  0.5367231471464038
feature loss:  0.5367230521515012
feature loss:  0.5367229809053242
feature loss:  0.5367228859104216
feature loss:  0.5367227671667933
feature loss:  0.5367226959206164
feature loss:  0.5367226009257138
feature loss:  0.5367225059308112
feature loss:  0.5367224346846342
feature loss:  0.5367222684435546
feature loss:  0.5367221496999264
feature loss:  0.5367220547050238
feature loss:  0.5367219359613955
feature loss:  0.5367219359613955
feature loss:  0.5367218884639442
feature loss:  0.5367217934690416
feature loss:  0.5367217222228646
feature loss:  0.5367216034792364
feature loss:  0.5367215084843338
feature loss:  0.5367214609868824
feature loss:  0.5367213659919798
feature loss:  0.5367212709970772
feature loss:  0.5367212709970772
feature loss:  0.5367212472483516
feature loss:  0.5367211047559977
feature loss:  0.536721081007272
feature loss:  0.5367210335098207
feature loss:  0.5367209860123694
feature loss:  0.5367209622636437
feature loss:  0.5367209147661924
feature loss:  0.5367208197712898
feature loss:  0.5367207722738385
feature loss:  0.5367206297814846
feature loss:  0.5367206535302103
feature loss:  0.5367205822840333
feature loss:  0.5367205585353076
feature loss:  0.5367204872891307
feature loss:  0.536720463540405
feature loss:  0.5367203922942281
feature loss:  0.5367203685455024
feature loss:  0.5367203447967768
feature loss:  0.5367202972993255
feature loss:  0.5367202498018742
feature loss:  0.5367201548069715
feature loss:  0.5367201310582459
feature loss:  0.5367200835607946
feature loss:  0.5367200123146176
feature loss:  0.5367200123146176
feature loss:  0.5367199648171663
feature loss:  0.536719917319715
feature loss:  0.5367198223248124
feature loss:  0.5367198460735381
feature loss:  0.5367197035811841
feature loss:  0.5367197035811841
feature loss:  0.5367196323350072
feature loss:  0.5367195848375559
feature loss:  0.5367195373401046
feature loss:  0.5367195135913789
feature loss:  0.5367195135913789
feature loss:  0.536719442345202
feature loss:  0.5367194660939276
feature loss:  0.536719371099025
feature loss:  0.5367194185964763
feature loss:  0.5367192523553967
feature loss:  0.5367192761041224
feature loss:  0.5367192523553967
feature loss:  0.5367191811092198
feature loss:  0.5367191098630428
feature loss:  0.5367191811092198
feature loss:  0.5367191098630428
feature loss:  0.5367190386168659
feature loss:  0.5367189911194146
feature loss:  0.5367189911194146
feature loss:  0.5367189436219633
feature loss:  0.536718896124512
feature loss:  0.5367188486270607
feature loss:  0.5367188723757863
feature loss:  0.5367187773808837
feature loss:  0.536718753632158
feature loss:  0.536718753632158
feature loss:  0.5367188011296093
feature loss:  0.5367187773808837
feature loss:  0.5367187298834324
feature loss:  0.5367186823859811
feature loss:  0.5367186586372554
feature loss:  0.5367186111398041
feature loss:  0.5367185636423528
feature loss:  0.5367185873910785
feature loss:  0.5367186111398041
feature loss:  0.5367184923961759
feature loss:  0.5367185636423528
feature loss:  0.5367184686474502
feature loss:  0.5367184686474502
feature loss:  0.5367184448987246
feature loss:  0.5367183974012733
feature loss:  0.5367183736525476
feature loss:  0.536718278657645
feature loss:  0.5367183974012733
feature loss:  0.536718349903822
feature loss:  0.5367182311601937
feature loss:  0.536718278657645
feature loss:  0.5367182549089193
feature loss:  0.5367182311601937
feature loss:  0.5367181836627424
feature loss:  0.5367181599140167
feature loss:  0.5367181836627424
feature loss:  0.5367181361652911
feature loss:  0.5367181361652911
feature loss:  0.5367181124165654
feature loss:  0.5367179936729372
feature loss:  0.5367180174216628
feature loss:  0.5367179936729372
feature loss:  0.5367179936729372
feature loss:  0.5367179699242115
feature loss:  0.5367179699242115
feature loss:  0.5367178511805832
feature loss:  0.5367177799344063
feature loss:  0.5367178274318576
feature loss:  0.5367178511805832
feature loss:  0.5367178274318576
feature loss:  0.5367178511805832
feature loss:  0.5367177561856806
feature loss:  0.5367177086882293
feature loss:  0.5367177561856806
feature loss:  0.5367176849395037
feature loss:  0.5367176849395037
feature loss:  0.5367176136933267
feature loss:  0.5367176849395037
feature loss:  0.5367176136933267
feature loss:  0.5367175899446011
feature loss:  0.5367176374420524
feature loss:  0.5367175899446011
feature loss:  0.5367176136933267
feature loss:  0.5367175186984241
feature loss:  0.5367174712009728
feature loss:  0.5367175186984241
feature loss:  0.5367174474522471
feature loss:  0.5367174237035215
feature loss:  0.5367174237035215
feature loss:  0.5367173999547958
feature loss:  0.5367174712009728
feature loss:  0.5367174237035215
feature loss:  0.5367173524573445
feature loss:  0.5367173999547958
feature loss:  0.5367173287086189
feature loss:  0.5367173762060702
feature loss:  0.5367172812111676
feature loss:  0.5367173049598932
feature loss:  0.5367172574624419
feature loss:  0.5367173524573445
feature loss:  0.5367172337137163
feature loss:  0.5367172574624419
feature loss:  0.5367172337137163
feature loss:  0.536717186216265
feature loss:  0.5367171624675393
feature loss:  0.536717114970088
feature loss:  0.536717114970088
feature loss:  0.536717114970088
feature loss:  0.536717114970088
feature loss:  0.536717114970088
feature loss:  0.5367170912213624
feature loss:  0.5367170199751854
feature loss:  0.5367170199751854
feature loss:  0.5367170674726367
feature loss:  0.5367170199751854
feature loss:  0.5367170199751854
feature loss:  0.5367170199751854
feature loss:  0.5367169962264597
feature loss:  0.5367169962264597
feature loss:  0.5367168774828315
feature loss:  0.5367169724777341
feature loss:  0.5367169249802828
feature loss:  0.5367169487290084
feature loss:  0.5367169962264597
feature loss:  0.5367168299853802
feature loss:  0.5367168537341058
feature loss:  0.5367168774828315
feature loss:  0.5367169012315571
feature loss:  0.5367167824879289
feature loss:  0.5367168774828315
feature loss:  0.5367167824879289
feature loss:  0.5367167824879289
feature loss:  0.5367167587392032
feature loss:  0.5367167824879289
feature loss:  0.5367167112417519
feature loss:  0.5367167587392032
feature loss:  0.5367167112417519
feature loss:  0.5367166874930263
feature loss:  0.5367167112417519
feature loss:  0.536716639995575
feature loss:  0.5367167112417519
feature loss:  0.5367167112417519
feature loss:  0.5367166637443006
feature loss:  0.536716639995575
feature loss:  0.5367165924981236
feature loss:  0.5367165924981236
feature loss:  0.5367165212519467
feature loss:  0.536716568749398
feature loss:  0.536716568749398
feature loss:  0.536716497503221
feature loss:  0.5367165450006723
feature loss:  0.5367164262570441
feature loss:  0.5367165212519467
feature loss:  0.536716497503221
feature loss:  0.5367164500057697
feature loss:  0.536716497503221
feature loss:  0.5367164500057697
feature loss:  0.5367164262570441
feature loss:  0.536716497503221
feature loss:  0.5367163787595928
feature loss:  0.5367163787595928
feature loss:  0.5367164025083184
feature loss:  0.5367163550108671
feature loss:  0.5367163550108671
feature loss:  0.5367163312621415
feature loss:  0.5367163550108671
feature loss:  0.5367162837646902
feature loss:  0.5367162837646902
feature loss:  0.5367162600159645
feature loss:  0.5367162837646902
feature loss:  0.5367163075134158
feature loss:  0.5367162362672389
feature loss:  0.5367162125185132
feature loss:  0.5367161650210619
feature loss:  0.5367161412723362
feature loss:  0.5367161887697875
feature loss:  0.5367161650210619
feature loss:  0.5367161650210619
feature loss:  0.5367162125185132
feature loss:  0.5367161650210619
feature loss:  0.5367160937748849
feature loss:  0.5367161412723362
feature loss:  0.5367161175236106
feature loss:  0.5367162125185132
feature loss:  0.5367160937748849
feature loss:  0.5367161175236106
feature loss:  0.5367160937748849
feature loss:  0.5367160937748849
feature loss:  0.5367160700261593
feature loss:  0.5367160937748849
feature loss:  0.5367160462774336
JobId=103610130 JobName=SimCLR
   UserId=renjie3(6007166) GroupId=cse(2010) MCS_label=N/A
   Priority=12117 Nice=0 Account=cmse QOS=normal
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=08:19:28 TimeLimit=1-06:00:00 TimeMin=N/A
   SubmitTime=2021-11-09T16:51:33 EligibleTime=2021-11-09T16:51:33
   AccrueTime=2021-11-09T16:51:33
   StartTime=2021-11-10T10:38:51 EndTime=2021-11-11T16:38:51 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2021-11-10T10:38:51
   Partition=cmse-gpu AllocNode:Sid=dev-amd20-v100:204881
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=nvl-001
   BatchHost=nvl-001
   FedOrigin=msuhpcc FedViableSiblings=msuhpcc,preemptable FedActiveSiblings=msuhpcc
   NumNodes=1 NumCPUs=2 NumTasks=1 CPUs/Task=2 ReqB:S:C:T=0:0:*:*
   TRES=cpu=2,mem=16G,node=1,billing=2490,gres/gpu=1
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=2 MinMemoryCPU=8G MinTmpDiskNode=0
   Features=[intel14|intel16|intel18|amr|nvf] DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/mnt/ufs18/home-145/renjie3/Documents/unlearnable/Unlearnable-Examples/submit.sb
   WorkDir=/mnt/ufs18/home-145/renjie3/Documents/unlearnable/Unlearnable-Examples
   Comment=stdout=/mnt/home/renjie3/Documents/unlearnable/Unlearnable-Examples/logfile/103610130.log 
   StdErr=/mnt/home/renjie3/Documents/unlearnable/Unlearnable-Examples/logfile/103610130.err
   StdIn=/dev/null
   StdOut=/mnt/home/renjie3/Documents/unlearnable/Unlearnable-Examples/logfile/103610130.log
   Power=
   TresPerNode=gpu:v100:1
   NtasksPerTRES:0


{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h2>Quick Start: Creating Sample-wise Unlearnable Examples</h2>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Prepare Data</h3>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Prepare Dataset\n",
    "train_transform = [\n",
    "    transforms.ToTensor()\n",
    "]\n",
    "test_transform = [\n",
    "    transforms.ToTensor()\n",
    "]\n",
    "train_transform = transforms.Compose(train_transform)\n",
    "test_transform = transforms.Compose(test_transform)\n",
    "\n",
    "clean_train_dataset = datasets.CIFAR10(root='../datasets', train=True, download=True, transform=train_transform)\n",
    "clean_test_dataset = datasets.CIFAR10(root='../datasets', train=False, download=True, transform=test_transform)\n",
    "\n",
    "clean_train_loader = DataLoader(dataset=clean_train_dataset, batch_size=512,\n",
    "                                shuffle=False, pin_memory=True,\n",
    "                                drop_last=False, num_workers=12)\n",
    "clean_test_loader = DataLoader(dataset=clean_test_dataset, batch_size=512,\n",
    "                                shuffle=False, pin_memory=True,\n",
    "                                drop_last=False, num_workers=12)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Prepare Model</h3>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from models.ResNet import ResNet18\n",
    "import toolbox\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "base_model = ResNet18()\n",
    "base_model = base_model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=base_model.parameters(), lr=0.1, weight_decay=0.0005, momentum=0.9)\n",
    "\n",
    "noise_generator = toolbox.PerturbationTool(epsilon=0.03137254901960784, num_steps=20, step_size=0.0031372549019607846)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Generate Error-Minimizing Noise</h3>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "noise = torch.zeros([50000, 3, 32, 32]) # reserved variable for noise\n",
    "data_iter = iter(clean_train_loader)\n",
    "condition = True\n",
    "train_idx = 0\n",
    "\n",
    "while condition:\n",
    "    # optimize theta for M steps, here we can see it is 10\n",
    "    base_model.train()\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = True\n",
    "    for j in range(0, 10):\n",
    "        try:\n",
    "            (images, labels) = next(data_iter)\n",
    "        except:\n",
    "            train_idx = 0\n",
    "            data_iter = iter(clean_train_loader)\n",
    "            (images, labels) = next(data_iter)\n",
    "        \n",
    "        for i, _ in enumerate(images):\n",
    "            # Update noise to images\n",
    "            images[i] += noise[train_idx]\n",
    "            train_idx += 1\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        base_model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        logits = base_model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(base_model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Perturbation over entire dataset\n",
    "    idx = 0\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for i, (images, labels) in tqdm(enumerate(clean_train_loader), total=len(clean_train_loader)):\n",
    "        batch_start_idx, batch_noise = idx, []\n",
    "        for i, _ in enumerate(images):\n",
    "            # Update noise to images\n",
    "            batch_noise.append(noise[idx])\n",
    "            idx += 1\n",
    "        batch_noise = torch.stack(batch_noise).cuda()\n",
    "        \n",
    "        # Update sample-wise perturbation\n",
    "        base_model.eval()\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        perturb_img, eta = noise_generator.min_min_attack(images, labels, base_model, optimizer, criterion, \n",
    "                                                          random_noise=batch_noise)\n",
    "        for i, delta in enumerate(eta):\n",
    "            noise[batch_start_idx+i] = delta.clone().detach().cpu()\n",
    "        \n",
    "    # Eval stop condition\n",
    "    eval_idx, total, correct = 0, 0, 0\n",
    "    for i, (images, labels) in enumerate(clean_train_loader):\n",
    "        for i, _ in enumerate(images):\n",
    "            # Update noise to images\n",
    "            images[i] += noise[eval_idx]\n",
    "            eval_idx += 1\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        with torch.no_grad():\n",
    "            logits = base_model(images)\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = correct / total\n",
    "    print('Accuracy %.2f' % (acc*100))\n",
    "    if acc > 0.99:\n",
    "        condition=False      \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Examine the noise\n",
    "print(noise)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Creat Unlearnable Dataset</h3>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "# Add standard augmentation\n",
    "train_transform = [\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "]\n",
    "train_transform = transforms.Compose(train_transform)\n",
    "clean_train_dataset = datasets.CIFAR10(root='../datasets', train=True, download=True, transform=train_transform)\n",
    "unlearnable_train_dataset = datasets.CIFAR10(root='../datasets', train=True, download=True, transform=train_transform)\n",
    "\n",
    "perturb_noise = noise.mul(255).clamp_(0, 255).permute(0, 2, 3, 1).to('cpu').numpy()\n",
    "unlearnable_train_dataset.data = unlearnable_train_dataset.data.astype(np.float32)\n",
    "for i in range(len(unlearnable_train_dataset)):\n",
    "    unlearnable_train_dataset.data[i] += perturb_noise[i]\n",
    "    unlearnable_train_dataset.data[i] = np.clip(unlearnable_train_dataset.data[i], a_min=0, a_max=255)\n",
    "unlearnable_train_dataset.data = unlearnable_train_dataset.data.astype(np.uint8)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Visualize Clean Images, Error-Minimizing Noise, Unlearnable Images</h3>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "def imshow(img):\n",
    "    fig = plt.figure(figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "def get_pairs_of_imgs(idx):\n",
    "    clean_img = clean_train_dataset.data[idx]\n",
    "    unlearnable_img = unlearnable_train_dataset.data[idx]\n",
    "    clean_img = torchvision.transforms.functional.to_tensor(clean_img)\n",
    "    unlearnable_img = torchvision.transforms.functional.to_tensor(unlearnable_img)\n",
    "\n",
    "    x = noise[idx]\n",
    "    x_min = torch.min(x)\n",
    "    x_max = torch.max(x)\n",
    "    noise_norm = (x - x_min) / (x_max - x_min)\n",
    "    noise_norm = torch.clamp(noise_norm, 0, 1)\n",
    "    return [clean_img, noise_norm, unlearnable_img]\n",
    "    \n",
    "selected_idx = [random.randint(0, 50000) for _ in range(3)]\n",
    "img_grid = []\n",
    "for idx in selected_idx:\n",
    "    img_grid += get_pairs_of_imgs(idx)\n",
    "    \n",
    "\n",
    "imshow(torchvision.utils.make_grid(torch.stack(img_grid), nrow=3, pad_value=255))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Train ResNet18 on Unlearnable Dataset</h3>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from util import AverageMeter\n",
    "\n",
    "model = ResNet18()\n",
    "model = model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.1, weight_decay=0.0005, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=0)\n",
    "\n",
    "unlearnable_loader = DataLoader(dataset=unlearnable_train_dataset, batch_size=128,\n",
    "                                shuffle=True, pin_memory=True,\n",
    "                                drop_last=False, num_workers=12)\n",
    "\n",
    "\n",
    "for epoch in range(30):\n",
    "    # Train\n",
    "    model.train()\n",
    "    acc_meter = AverageMeter()\n",
    "    loss_meter = AverageMeter()\n",
    "    pbar = tqdm(unlearnable_loader, total=len(unlearnable_loader))\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        acc = (predicted == labels).sum().item()/labels.size(0)\n",
    "        acc_meter.update(acc)\n",
    "        loss_meter.update(loss.item())\n",
    "        pbar.set_description(\"Acc %.2f Loss: %.2f\" % (acc_meter.avg*100, loss_meter.avg))\n",
    "    scheduler.step()\n",
    "    # Eval\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for i, (images, labels) in enumerate(clean_test_loader):\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        with torch.no_grad():\n",
    "            logits = model(images)\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = correct / total\n",
    "    tqdm.write('Clean Accuracy %.2f\\n' % (acc*100))\n",
    "            "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a289a4ff3d2b3bf9a8171309228094dd1c9285b488c809e0985bf7cd7684634d"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('py37': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}